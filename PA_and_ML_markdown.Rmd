---
title: "Predictive analytics and machine learning in R"
author: "Myffy"
date: "1/3/2017"
output: html_document
---
Data used to build predictive model
Default of Credit Card Clients in Tiawan
https://archive.ics.uci.edu/ml/machine-learning-databases/00350/
Paper for Background : Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library(data.table)
library(plyr)
library(gmodels)
library(tree)
library(tables)
library(corrplot)
library(randomForest)
library(ROCR)
library(caret)
```

Read in data and view

```{r, echo=TRUE}
credit<- read.csv("/Users/mhhopkin/Documents/general work 2015/R presentation/default of credit card clients.csv")
#credit<- read.csv("~/default of credit card clients.csv")
#summary(credit)
str(credit)
```
Data summary. Note the reverse of logical month series. 

This research aimed at the case of customer default payments in Taiwan and builds a predictive model for probability of default.
This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables: 

X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. 

X2: Gender (1 = male; 2 = female). 

X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). 

X4: Marital status (1 = married; 2 = single; 3 = others). 

X5: Age (year). 

X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. 

X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. 

X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. 

First we must re-order the varibles in a more logical order for computation, then re-lable them for better understanding.
```{r, echo=TRUE}
credit2 <- credit[c(1:6,12,11,10,9,8,7,18,17,16,15,14,13,24,23,22,21,20,19, 25)]
names(credit2)[7:25] <- c( "PAY_April","PAY_May","PAY_June","PAY_July","PAY_Aug", "PAY_Sept","BILL_AMT_April",
                           "BILL_AMT_May", "BILL_AMT_June", "BILL_AMT_July", "BILL_AMT_Aug", "BILL_AMT_Sept" ,
                           "PAY_AMT_April" , "PAY_AMT_May" , "PAY_AMT_June"  ,"PAY_AMT_July" , "PAY_AMT_Aug" , 
                           "PAY_AMT_Sept","default")
names(credit2)

```
Feature engineering: 

Now we can brainstorm about generting predictive features out of the available features. There are also algorthims that will generate  features. rather than generating thousands of useless varibles, I like ot use domain knowldege to generate the most likely uselful combinations of features. adding, subtrating, multiply/divide and exponents/logs are a great place to start. 

Features to test: 

1. Did the payment increase or decrease in the most recent month? (Sept payment- Aug payment)

2. What percentage of the overall balence was paid in August?

3. What percentage of the overall balence was paid in Sept?
```{r, echo=TRUE}
#feature: credit card balence in September
credit2$last_month <- (credit2$PAY_AMT_Sept-credit2$PAY_AMT_Aug)
credit2$percent_pay_aug <- ifelse(credit2$BILL_AMT_Sept==0,0,(credit2$PAY_AMT_Aug/credit2$BILL_AMT_Sept))
credit2$percent_pay_sept <- ifelse(credit2$BILL_AMT_Sept==0,0,(credit2$PAY_AMT_Sept/credit2$BILL_AMT_Sept))
#also will convert numeric (integer) varibles which are really categorical into factors
credit2$SEX <-as.factor(credit2$SEX )
credit2$EDUCATION <- as.factor(credit2$EDUCATION)
credit2$MARRIAGE  <- as.factor(credit2$MARRIAGE)
credit2$default  <- as.factor(credit2$default)
```
Correlation of features (can alter model performance, variable autocorrelations are important to understand)
```{r, message=FALSE, warning=FALSE}
#varibles with p> 0.01 are left blank. Only significant correlations are shown in color. Color intensity is proportional to the correlation coefficient.
#library(corrplot)
res <- cor(credit2[, c(2,6:24,26:28)])
#round(res, 2)
plot1<-corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

```

Create training (60%), CV (20%), and test (20%) datasets

```{r}
smp_size <- floor(0.6 * nrow(credit2))
## set the seed to make your partition reproductible
set.seed(345)
train_ind <- sample(seq_len(nrow(credit2)), size = smp_size)
train <- credit2[train_ind, ]
split <- credit2[-train_ind, ]

smp_size2 <- floor(0.5 * nrow(split))
## set the seed to make your partition reproductible, change seed to get different data partitions
set.seed(456)
train_ind2 <- sample(seq_len(nrow(split)), size = smp_size2)
CV <- split[train_ind2, ]
test <- split[-train_ind2, ]

train<- train[c(2:28)]
CV<- CV[c(2:28)]
test<- test[c(2:28)]
```

Lets try the Random forest model on the data and loop over all possible values of mtry (#of parameters in the model)
The number of trees can also be looped. I have chosed only 100 trees to save on computation time.
```{r, echo=TRUE}
#library(randomForest)
#library(ROCR)

#for(i in 1:26){   #using for loop to loop over mtry values
set.seed(123)  #use set.seed to get a reproducible RF model
#RandomForestROC <- function (y1, y2, train, testset)
i=9
bag = randomForest(as.factor(default) ~., data=train, mtry=i, ntree=100, importance=TRUE)
## type="prob" generates probabilities instead of class labels  
##randomForest generates probabilities for both the class labels, therfore
##we are selecting one of the value [2] which selects the correct probability
bag.pr = predict(bag, type="prob",newdata=CV)[,2]
##prediction is ROCR function
bag.pred = prediction(bag.pr, CV$default)
##performance in terms of true and false positive rates
bag.perf = performance(bag.pred,"tpr","fpr")
auc <- performance(bag.pred ,"auc");auc <- unlist(slot(auc, "y.values")); auc<-max(round(auc, digits = 4));
print(paste(c("AUC =",auc,"mtry=",9)))
#}
 
```
[1] "AUC ="  "0.7393" "mtry="  "1"     
[1] "AUC =" "0.749" "mtry=" "2"    
[1] "AUC ="  "0.7541" "mtry="  "3"     
[1] "AUC ="  "0.7569" "mtry="  "4"     
[1] "AUC ="  "0.7588" "mtry="  "5"     
[1] "AUC =" "0.753" "mtry=" "6"    
[1] "AUC ="  "0.7583" "mtry="  "7"     
[1] "AUC ="  "0.7598" "mtry="  "8"     
[1] "AUC ="  "0.7605" "mtry="  "9"     
[1] "AUC ="  "0.7601" "mtry="  "10"    
[1] "AUC ="  "0.7587" "mtry="  "11"    
[1] "AUC ="  "0.7547" "mtry="  "12"    
[1] "AUC ="  "0.7595" "mtry="  "13"    
[1] "AUC ="  "0.7566" "mtry="  "14"    
[1] "AUC ="  "0.7569" "mtry="  "15"    
[1] "AUC ="  "0.7557" "mtry="  "16"    
[1] "AUC ="  "0.7575" "mtry="  "17"    
[1] "AUC ="  "0.7547" "mtry="  "18"    
[1] "AUC ="  "0.7598" "mtry="  "19"    
[1] "AUC ="  "0.7569" "mtry="  "20"    
[1] "AUC ="  "0.7549" "mtry="  "21"    
[1] "AUC ="  "0.7576" "mtry="  "22"    
[1] "AUC ="  "0.7554" "mtry="  "23"    
[1] "AUC ="  "0.7567" "mtry="  "24"    
[1] "AUC ="  "0.7582" "mtry="  "25"    
[1] "AUC ="  "0.7555" "mtry="  "26"   


Model performance is very consistant. Pick the mtry value with the highest AUC, 9 in this case. 

Now we can look at variable importance and try to create the most parsimonious (fewest varibles with best performance) model
```{r}
#importance(bag)
#quartz()
varImpPlot(bag, type=1, pch=19, col=1, cex=0.75, main="")
```

Lower performing variables can be removed, and model performance can be tested; many interations can result

```{r}
#for example:
train2<- train[c(1,4:5,11,17:27)]
CV2<- CV[c(1,4:5,10:12,17:27)]
test2<- test[c(1,4:5,10:12,17:27)]
```

Now lets look at the ROC curve to dertermine fit and best probability cutoff for the model prediction
```{r}
#quartz()
par(cex.axis=1.25, cex.lab=1.5, cex.main=1.5)
plot(bag.perf, colorize=TRUE,lwd=2.5, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7), 
     main="ROC Curve for Random Forest, Credit card default in October, 2015, Taiwan")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```
Look at false positives and false negatives based on designated cutoff

```{r}
#library(caret)
#pick a cutoff in the elbow of the ROC curve
cutoff=c(0.8,0.2)
set.seed(123)
#bag2 = randomForest(as.factor(default) ~., data=train, mtry=9, ntree=100, importance=TRUE, cutoff=cutoff)
pred2 <- predict(bag2, newdata=CV)
CrossTable(pred2, CV$default)
specificity(pred2, CV$default, negative = levels(CV$default)[1])
sensitivity(pred2, CV$default, positive = levels(CV$default)[-1])

#Trying different cutoffs can addjust  sensitivity and specificity to desired levels of false positives vs false negatives
cutoff=c(0.7,0.3)
set.seed(123)
#bag3 = randomForest(as.factor(default) ~., data=train, mtry=9, ntree=100, importance=TRUE, cutoff=cutoff)
pred3 <- predict(bag3, newdata=CV)
CrossTable(pred3, CV$default)
specificity(pred3, CV$default, negative = levels(CV$default)[1])
sensitivity(pred3, CV$default, positive = levels(CV$default)[-1])
```
Finally, report out the model performance on the test data:

```{r, echo=TRUE}
trainall<- rbind(train,CV) #traina nd CV datasets can be combined to create a larger dataset for training

cutoff=c(0.7,0.3)
set.seed(123)
#bag5 = randomForest(as.factor(default) ~., data=trainall, mtry=9, ntree=100, importance=TRUE, cutoff=cutoff)
pred5 <- predict(bag5, newdata=test)
CrossTable(pred5, test$default)
specificity(pred5, test$default, negative = levels(test$default)[1])
sensitivity(pred5, test$default, positive = levels(test$default)[-1])

```
